<configuration>
  <property>
    <name>io.map.index.skip</name>
    <value>0</value>
  </property>
  <property>
    <name>io.seqfile.compress.blocksize</name>
    <value>1000000</value>
  </property>
  <property>
    <name>s3.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>hadoop.security.groups.cache.secs</name>
    <value>300</value>
  </property>
  <property>
    <name>hadoop.jetty.logs.serve.aliases</name>
    <value>true</value>
  </property>
  <property>
    <name>ha.failover-controller.graceful-fence.rpc-timeout.ms</name>
    <value>5000</value>
  </property>
  <property>
    <name>hadoop.http.filter.initializers</name>
    <value>org.apache.hadoop.http.lib.StaticUserWebFilter</value>
  </property>
  <property>
    <name>ftp.blocksize</name>
    <value>67108864</value>
  </property>
  <property>
    <name>ipc.server.listen.queue.size</name>
    <value>128</value>
  </property>
  <property>
    <name>hadoop.http.authentication.kerberos.keytab</name>
    <value>${user.home}/hadoop.keytab</value>
  </property>
  <property>
    <name>hadoop.ssl.hostname.verifier</name>
    <value>DEFAULT</value>
  </property>
  <property>
    <name>hadoop.ssl.client.conf</name>
    <value>ssl-client.xml</value>
  </property>
  <property>
    <name>hadoop.security.group.mapping.ldap.search.attr.member</name>
    <value>member</value>
  </property>
  <property>
    <name>hadoop.security.authorization</name>
    <value>false</value>
  </property>
  <property>
    <name>hadoop.ssl.enabled</name>
    <value>false</value>
  </property>
  <property>
    <name>fs.ftp.host</name>
    <value>0.0.0.0</value>
  </property>
  <property>
    <name>fs.AbstractFileSystem.hdfs.impl</name>
    <value>org.apache.hadoop.fs.Hdfs</value>
  </property>
  <property>
    <name>ha.failover-controller.new-active.rpc-timeout.ms</name>
    <value>60000</value>
  </property>
  <property>
    <name>ha.health-monitor.check-interval.ms</name>
    <value>1000</value>
  </property>
  <property>
    <name>io.file.buffer.size</name>
    <value>65536</value>
  </property>
  <property>
    <name>ha.zookeeper.session-timeout.ms</name>
    <value>5000</value>
  </property>
  <property>
    <name>hadoop.ssl.keystores.factory.class</name>
    <value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value>
  </property>
  <property>
    <name>ipc.client.kill.max</name>
    <value>10</value>
  </property>
  <property>
    <name>kfs.stream-buffer-size</name>
    <value>4096</value>
  </property>
  <property>
    <name>hadoop.ssl.require.client.cert</name>
    <value>false</value>
  </property>
  <property>
    <name>ftp.client-write-packet-size</name>
    <value>65536</value>
  </property>
  <property>
    <name>ipc.server.tcpnodelay</name>
    <value>false</value>
  </property>
  <property>
    <name>s3.stream-buffer-size</name>
    <value>4096</value>
  </property>
  <property>
    <name>ha.health-monitor.connect-retry-interval.ms</name>
    <value>1000</value>
  </property>
  <property>
    <name>file.blocksize</name>
    <value>67108864</value>
  </property>
  <property>
    <name>fs.ftp.host.port</name>
    <value>21</value>
  </property>
  <property>
    <name>hadoop.http.authentication.token.validity</name>
    <value>36000</value>
  </property>
  <property>
    <name>s3.client-write-packet-size</name>
    <value>65536</value>
  </property>
  <property>
    <name>s3native.blocksize</name>
    <value>67108864</value>
  </property>
  <property>
    <name>hadoop.security.group.mapping.ldap.search.filter.group</name>
    <value>(objectClass=group)</value>
  </property>
  <property>
    <name>ipc.client.connect.max.retries.on.timeouts</name>
    <value>45</value>
  </property>
  <property>
    <name>ha.zookeeper.acl</name>
    <value>world:anyone:rwcda</value>
  </property>
  <property>
    <name>kfs.client-write-packet-size</name>
    <value>65536</value>
  </property>
  <property>
    <name>hadoop.security.uid.cache.secs</name>
    <value>14400</value>
  </property>
  <property>
    <name>fs.trash.checkpoint.interval</name>
    <value>0</value>
  </property>
  <property>
    <name>s3.blocksize</name>
    <value>67108864</value>
  </property>
  <property>
    <name>kfs.blocksize</name>
    <value>67108864</value>
  </property>
  <property>
    <name>fs.AbstractFileSystem.file.impl</name>
    <value>org.apache.hadoop.fs.local.LocalFs</value>
  </property>
  <property>
    <name>file.stream-buffer-size</name>
    <value>4096</value>
  </property>
  <property>
    <name>ftp.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>ipc.client.connection.maxidletime</name>
    <value>10000</value>
  </property>
  <property>
    <name>fs.s3n.block.size</name>
    <value>67108864</value>
  </property>
  <property>
    <name>hadoop.security.group.mapping.ldap.search.filter.user</name>
    <value>(&amp;(objectClass=user)(sAMAccountName={0}))</value>
  </property>
  <property>
    <name>hadoop.http.staticuser.user</name>
    <value>dr.who</value>
  </property>
  <property>
    <name>hadoop.security.auth_to_local</name>
    <value>DEFAULT</value>
  </property>
  <property>
    <name>kfs.bytes-per-checksum</name>
    <value>512</value>
  </property>
  <property>
    <name>hadoop.rpc.protection</name>
    <value>authentication</value>
  </property>
  <property>
    <name>hadoop.security.instrumentation.requires.admin</name>
    <value>false</value>
  </property>
  <property>
    <name>ha.zookeeper.parent-znode</name>
    <value>/hadoop-ha</value>
  </property>
  <property>
    <name>ftp.stream-buffer-size</name>
    <value>4096</value>
  </property>
  <property>
    <name>hadoop.http.authentication.signature.secret.file</name>
    <value>${user.home}/hadoop-http-auth-signature-secret</value>
  </property>
  <property>
    <name>io.skip.checksum.errors</name>
    <value>false</value>
  </property>
  <property>
    <name>s3native.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>hadoop.http.authentication.kerberos.principal</name>
    <value>HTTP/_HOST@LOCALHOST</value>
  </property>
  <property>
    <name>fs.s3.maxRetries</name>
    <value>4</value>
  </property>
  <property>
    <name>file.client-write-packet-size</name>
    <value>65536</value>
  </property>
  <property>
    <name>file.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>hadoop.http.authentication.simple.anonymous.allowed</name>
    <value>true</value>
  </property>
  <property>
    <name>hadoop.kerberos.kinit.command</name>
    <value>kinit</value>
  </property>
  <property>
    <name>ipc.client.idlethreshold</name>
    <value>4000</value>
  </property>
  <property>
    <name>tfile.fs.input.buffer.size</name>
    <value>262144</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/tmp/hadoop-${user.name}</value>
  </property>
  <property>
    <name>tfile.io.chunk.size</name>
    <value>1048576</value>
  </property>
  <property>
    <name>ha.failover-controller.cli-check.rpc-timeout.ms</name>
    <value>20000</value>
  </property>
  <property>
    <name>fs.s3.block.size</name>
    <value>67108864</value>
  </property>
  <property>
    <name>io.serializations</name>
    <value>org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization</value>
  </property>
  <property>
    <name>hadoop.util.hash.type</name>
    <value>murmur</value>
  </property>
  <property>
    <name>io.seqfile.lazydecompress</name>
    <value>true</value>
  </property>
  <property>
    <name>io.mapfile.bloom.size</name>
    <value>1048576</value>
  </property>
  <property>
    <name>fs.s3.buffer.dir</name>
    <value>${hadoop.tmp.dir}/s3</value>
  </property>
  <property>
    <name>dfs.ha.fencing.ssh.connect-timeout</name>
    <value>30000</value>
  </property>
  <property>
    <name>s3.bytes-per-checksum</name>
    <value>512</value>
  </property>
  <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec</value>
  </property>
  <property>
    <name>fs.automatic.close</name>
    <value>true</value>
  </property>
  <property>
    <name>hadoop.work.around.non.threadsafe.getpwuid</name>
    <value>false</value>
  </property>
  <property>
    <name>s3native.client-write-packet-size</name>
    <value>65536</value>
  </property>
  <property>
    <name>io.seqfile.sorter.recordlimit</name>
    <value>1000000</value>
  </property>
  <property>
    <name>hadoop.security.group.mapping.ldap.search.attr.group.name</name>
    <value>cn</value>
  </property>
  <property>
    <name>fs.trash.interval</name>
    <value>1</value>
  </property>
  <property>
    <name>fs.df.interval</name>
    <value>60000</value>
  </property>
  <property>
    <name>hadoop.security.authentication</name>
    <value>simple</value>
  </property>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost.localdomain:8020</value>
  </property>
  <property>
    <name>hadoop.security.group.mapping</name>
    <value>org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback</value>
  </property>
  <property>
    <name>ftp.bytes-per-checksum</name>
    <value>512</value>
  </property>
  <property>
    <name>ipc.client.connect.max.retries</name>
    <value>10</value>
  </property>
  <property>
    <name>hadoop.ssl.server.conf</name>
    <value>ssl-server.xml</value>
  </property>
  <property>
    <name>ha.health-monitor.sleep-after-disconnect.ms</name>
    <value>1000</value>
  </property>
  <property>
    <name>hadoop.rpc.socket.factory.class.default</name>
    <value>org.apache.hadoop.net.StandardSocketFactory</value>
  </property>
  <property>
    <name>net.topology.node.switch.mapping.impl</name>
    <value>org.apache.hadoop.net.ScriptBasedMapping</value>
  </property>
  <property>
    <name>hadoop.security.group.mapping.ldap.ssl</name>
    <value>false</value>
  </property>
  <property>
    <name>net.topology.script.number.args</name>
    <value>100</value>
  </property>
  <property>
    <name>fs.AbstractFileSystem.viewfs.impl</name>
    <value>org.apache.hadoop.fs.viewfs.ViewFs</value>
  </property>
  <property>
    <name>io.map.index.interval</name>
    <value>128</value>
  </property>
  <property>
    <name>io.native.lib.available</name>
    <value>true</value>
  </property>
  <property>
    <name>s3native.stream-buffer-size</name>
    <value>4096</value>
  </property>
  <property>
    <name>tfile.fs.output.buffer.size</name>
    <value>262144</value>
  </property>
  <property>
    <name>fs.permissions.umask-mode</name>
    <value>022</value>
  </property>
  <property>
    <name>io.mapfile.bloom.error.rate</name>
    <value>0.005</value>
  </property>
  <property>
    <name>io.bytes.per.checksum</name>
    <value>512</value>
  </property>
  <property>
    <name>ha.failover-controller.graceful-fence.connection.retries</name>
    <value>1</value>
  </property>
  <property>
    <name>ipc.client.tcpnodelay</name>
    <value>false</value>
  </property>
  <property>
    <name>s3native.bytes-per-checksum</name>
    <value>512</value>
  </property>
  <property>
    <name>ha.health-monitor.rpc-timeout.ms</name>
    <value>45000</value>
  </property>
  <property>
    <name>kfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>hadoop.common.configuration.version</name>
    <value>0.23.0</value>
  </property>
  <property>
    <name>io.seqfile.local.dir</name>
    <value>${hadoop.tmp.dir}/io/local</value>
  </property>
  <property>
    <name>file.bytes-per-checksum</name>
    <value>512</value>
  </property>
  <property>
    <name>fs.s3.sleepTimeSeconds</name>
    <value>10</value>
  </property>
  <property>
    <name>hadoop.http.authentication.type</name>
    <value>simple</value>
  </property>
</configuration>
